# -*- coding: utf-8 -*-
"""FNN-without-weightinitialization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RDfV7inj7TB-lOxJMqwxxUL9ad47cjkx

IMPORT LIBRARIES
"""

# Train and evaluate the baseline model
baseline_model = build_baseline_model()
history_baseline, test_loss_baseline, test_acc_baseline = compile_fit_evaluate_model(
    baseline_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Xavier initialization model
xavier_model = build_xavier_model()
history_xavier, test_loss_xavier, test_acc_xavier = compile_fit_evaluate_model(
    xavier_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Kaiming initialization model
kaiming_model = build_kaiming_model()
history_kaiming, test_loss_kaiming, test_acc_kaiming = compile_fit_evaluate_model(
    kaiming_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the dropout regularization model
dropout_model = build_dropout_model()
history_dropout, test_loss_dropout, test_acc_dropout = compile_fit_evaluate_model(
    dropout_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the L2 regularization model
regularized_model = build_regularized_model()
history_regularized, test_loss_regularized, test_acc_regularized = compile_fit_evaluate_model(
    regularized_model, x_train, y_train, x_test, y_test
)

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import regularizers
import matplotlib.pyplot as plt

# Load and preprocess data
cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train, y_test = y_train.flatten(), y_test.flatten()


# Define model creation functions
def build_baseline_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_xavier_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(64, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(32, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
    ])
    return model

def build_kaiming_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(10, activation='softmax', kernel_initializer='he_normal')
    ])
    return model

def build_dropout_model(dropout_rate=0.3):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(64, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(32, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_regularized_model(l2_factor=0.001):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(10, activation='softmax') # Regularization is typically not applied to the output layer
    ])
    return model

# Define training and evaluation function
def compile_fit_evaluate_model(model, x_train, y_train, x_test, y_test):
    """Compiles, fits, and evaluates a Keras model."""
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        x_train, y_train,
        validation_data=(x_test, y_test),
        epochs=10,
        batch_size=128,
        verbose=0 # Set verbose to 0 to avoid printing progress bar for each epoch
    )

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

    return history, test_loss, test_acc

# Train and evaluate the baseline model
baseline_model = build_baseline_model()
history_baseline, test_loss_baseline, test_acc_baseline = compile_fit_evaluate_model(
    baseline_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Xavier initialization model
xavier_model = build_xavier_model()
history_xavier, test_loss_xavier, test_acc_xavier = compile_fit_evaluate_model(
    xavier_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Kaiming initialization model
kaiming_model = build_kaiming_model()
history_kaiming, test_loss_kaiming, test_acc_kaiming = compile_fit_evaluate_model(
    kaiming_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the dropout regularization model
dropout_model = build_dropout_model()
history_dropout, test_loss_dropout, test_acc_dropout = compile_fit_evaluate_model(
    dropout_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the L2 regularization model
regularized_model = build_regularized_model()
history_regularized, test_loss_regularized, test_acc_regularized = compile_fit_evaluate_model(
    regularized_model, x_train, y_train, x_test, y_test
)

# Visualize the results
plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['accuracy'], label='Baseline Training Accuracy')
plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')
plt.plot(history_xavier.history['accuracy'], label='Xavier Training Accuracy')
plt.plot(history_xavier.history['val_accuracy'], label='Xavier Validation Accuracy')
plt.plot(history_kaiming.history['accuracy'], label='Kaiming Training Accuracy')
plt.plot(history_kaiming.history['val_accuracy'], label='Kaiming Validation Accuracy')
plt.plot(history_dropout.history['accuracy'], label='Dropout Training Accuracy')
plt.plot(history_dropout.history['val_accuracy'], label='Dropout Validation Accuracy')
plt.plot(history_regularized.history['accuracy'], label='Regularized Training Accuracy')
plt.plot(history_regularized.history['val_accuracy'], label='Regularized Validation Accuracy')
plt.title('Model Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim([0, 1])
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['loss'], label='Baseline Training Loss')
plt.plot(history_baseline.history['val_loss'], label='Baseline Validation Loss')
plt.plot(history_xavier.history['loss'], label='Xavier Training Loss')
plt.plot(history_xavier.history['val_loss'], label='Xavier Validation Loss')
plt.plot(history_kaiming.history['loss'], label='Kaiming Training Loss')
plt.plot(history_kaiming.history['val_loss'], label='Kaiming Validation Loss')
plt.plot(history_dropout.history['loss'], label='Dropout Training Loss')
plt.plot(history_dropout.history['val_loss'], label='Dropout Validation Loss')
plt.plot(history_regularized.history['loss'], label='Regularized Training Loss')
plt.plot(history_regularized.history['val_loss'], label='Regularized Validation Loss')
plt.title('Model Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

cifar10 = tf.keras.datasets.cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train, y_test = y_train.flatten(), y_test.flatten()

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import regularizers

def build_baseline_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_xavier_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(64, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(32, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
    ])
    return model

def build_kaiming_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(10, activation='softmax', kernel_initializer='he_normal')
    ])
    return model

def build_dropout_model(dropout_rate=0.3):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(64, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(32, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_regularized_model(l2_factor=0.001):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(10, activation='softmax') # Regularization is typically not applied to the output layer
    ])
    return model

def compile_fit_evaluate_model(model, x_train, y_train, x_test, y_test):
    """Compiles, fits, and evaluates a Keras model."""
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        x_train, y_train,
        validation_data=(x_test, y_test),
        epochs=10,
        batch_size=128,
        verbose=0 # Set verbose to 0 to avoid printing progress bar for each epoch
    )

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

    return history, test_loss, test_acc

"""## Define training and evaluation function

### Subtask:
Create a function that compiles, fits, and evaluates a given model.
"""

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import regularizers

def build_baseline_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_xavier_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(64, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(32, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
    ])
    return model

def build_kaiming_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(10, activation='softmax', kernel_initializer='he_normal')
    ])
    return model

def build_dropout_model(dropout_rate=0.3):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(64, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(32, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_regularized_model(l2_factor=0.001):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(10, activation='softmax') # Regularization is typically not applied to the output layer
    ])
    return model

"""## Define model creation functions

### Subtask:
Create separate functions to build the model with the following configurations:
- Baseline model (default initializer)
- Model with Xavier initialization (`glorot_uniform`)
- Model with Kaiming initialization (`he_normal`)
- Model with dropout regularization (apply dropout to hidden layers)
- Model with L1 or L2 regularization (add regularization to dense layers)
"""

cifar10 = tf.keras.datasets.cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train, y_test = y_train.flatten(), y_test.flatten()

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import regularizers

def build_baseline_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_xavier_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(64, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(32, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
    ])
    return model

def build_kaiming_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(10, activation='softmax', kernel_initializer='he_normal')
    ])
    return model

def build_dropout_model(dropout_rate=0.3):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(64, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(32, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_regularized_model(l2_factor=0.001):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(10, activation='softmax') # Regularization is typically not applied to the output layer
    ])
    return model

def compile_fit_evaluate_model(model, x_train, y_train, x_test, y_test):
    """Compiles, fits, and evaluates a Keras model."""
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        x_train, y_train,
        validation_data=(x_test, y_test),
        epochs=10,
        batch_size=128,
        verbose=0 # Set verbose to 0 to avoid printing progress bar for each epoch
    )

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

    return history, test_loss, test_acc

# Train and evaluate the baseline model
baseline_model = build_baseline_model()
history_baseline, test_loss_baseline, test_acc_baseline = compile_fit_evaluate_model(
    baseline_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Xavier initialization model
xavier_model = build_xavier_model()
history_xavier, test_loss_xavier, test_acc_xavier = compile_fit_evaluate_model(
    xavier_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Kaiming initialization model
kaiming_model = build_kaiming_model()
history_kaiming, test_loss_kaiming, test_acc_kaiming = compile_fit_evaluate_model(
    kaiming_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the dropout regularization model
dropout_model = build_dropout_model()
history_dropout, test_loss_dropout, test_acc_dropout = compile_fit_evaluate_model(
    dropout_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the L2 regularization model
regularized_model = build_regularized_model()
history_regularized, test_loss_regularized, test_acc_regularized = compile_fit_evaluate_model(
    regularized_model, x_train, y_train, x_test, y_test
)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['accuracy'], label='Baseline Training Accuracy')
plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')
plt.plot(history_xavier.history['accuracy'], label='Xavier Training Accuracy')
plt.plot(history_xavier.history['val_accuracy'], label='Xavier Validation Accuracy')
plt.plot(history_kaiming.history['accuracy'], label='Kaiming Training Accuracy')
plt.plot(history_kaiming.history['val_accuracy'], label='Kaiming Validation Accuracy')
plt.plot(history_dropout.history['accuracy'], label='Dropout Training Accuracy')
plt.plot(history_dropout.history['val_accuracy'], label='Dropout Validation Accuracy')
plt.plot(history_regularized.history['accuracy'], label='Regularized Training Accuracy')
plt.plot(history_regularized.history['val_accuracy'], label='Regularized Validation Accuracy')
plt.title('Model Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim([0, 1])
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['loss'], label='Baseline Training Loss')
plt.plot(history_baseline.history['val_loss'], label='Baseline Validation Loss')
plt.plot(history_xavier.history['loss'], label='Xavier Training Loss')
plt.plot(history_xavier.history['val_loss'], label='Xavier Validation Loss')
plt.plot(history_kaiming.history['loss'], label='Kaiming Training Loss')
plt.plot(history_kaiming.history['val_loss'], label='Kaiming Validation Loss')
plt.plot(history_dropout.history['loss'], label='Dropout Training Loss')
plt.plot(history_dropout.history['val_loss'], label='Dropout Validation Loss')
plt.plot(history_regularized.history['loss'], label='Regularized Training Loss')
plt.plot(history_regularized.history['val_loss'], label='Regularized Validation Loss')
plt.title('Model Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import regularizers

def build_baseline_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_xavier_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(64, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(32, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
    ])
    return model

def build_kaiming_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(10, activation='softmax', kernel_initializer='he_normal')
    ])
    return model

def build_dropout_model(dropout_rate=0.3):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(64, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(32, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_regularized_model(l2_factor=0.001):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(10, activation='softmax') # Regularization is typically not applied to the output layer
    ])
    return model

def compile_fit_evaluate_model(model, x_train, y_train, x_test, y_test):
    """Compiles, fits, and evaluates a Keras model."""
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        x_train, y_train,
        validation_data=(x_test, y_test),
        epochs=10,
        batch_size=128,
        verbose=0 # Set verbose to 0 to avoid printing progress bar for each epoch
    )

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

    return history, test_loss, test_acc

# Train and evaluate the baseline model
baseline_model = build_baseline_model()
history_baseline, test_loss_baseline, test_acc_baseline = compile_fit_evaluate_model(
    baseline_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Xavier initialization model
xavier_model = build_xavier_model()
history_xavier, test_loss_xavier, test_acc_xavier = compile_fit_evaluate_model(
    xavier_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Kaiming initialization model
kaiming_model = build_kaiming_model()
history_kaiming, test_loss_kaiming, test_acc_kaiming = compile_fit_evaluate_model(
    kaiming_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the dropout regularization model
dropout_model = build_dropout_model()
history_dropout, test_loss_dropout, test_acc_dropout = compile_fit_evaluate_model(
    dropout_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the L2 regularization model
regularized_model = build_regularized_model()
history_regularized, test_loss_regularized, test_acc_regularized = compile_fit_evaluate_model(
    regularized_model, x_train, y_train, x_test, y_test
)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['accuracy'], label='Baseline Training Accuracy')
plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')
plt.plot(history_xavier.history['accuracy'], label='Xavier Training Accuracy')
plt.plot(history_xavier.history['val_accuracy'], label='Xavier Validation Accuracy')
plt.plot(history_kaiming.history['accuracy'], label='Kaiming Training Accuracy')
plt.plot(history_kaiming.history['val_accuracy'], label='Kaiming Validation Accuracy')
plt.plot(history_dropout.history['accuracy'], label='Dropout Training Accuracy')
plt.plot(history_dropout.history['val_accuracy'], label='Dropout Validation Accuracy')
plt.plot(history_regularized.history['accuracy'], label='Regularized Training Accuracy')
plt.plot(history_regularized.history['val_accuracy'], label='Regularized Validation Accuracy')
plt.title('Model Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim([0, 1])
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['loss'], label='Baseline Training Loss')
plt.plot(history_baseline.history['val_loss'], label='Baseline Validation Loss')
plt.plot(history_xavier.history['loss'], label='Xavier Training Loss')
plt.plot(history_xavier.history['val_loss'], label='Xavier Validation Loss')
plt.plot(history_kaiming.history['loss'], label='Kaiming Training Loss')
plt.plot(history_kaiming.history['val_loss'], label='Kaiming Validation Loss')
plt.plot(history_dropout.history['loss'], label='Dropout Training Loss')
plt.plot(history_dropout.history['val_loss'], label='Dropout Validation Loss')
plt.plot(history_regularized.history['loss'], label='Regularized Training Loss')
plt.plot(history_regularized.history['val_loss'], label='Regularized Validation Loss')
plt.title('Model Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Train and evaluate the baseline model
baseline_model = build_baseline_model()
history_baseline, test_loss_baseline, test_acc_baseline = compile_fit_evaluate_model(
    baseline_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Xavier initialization model
xavier_model = build_xavier_model()
history_xavier, test_loss_xavier, test_acc_xavier = compile_fit_evaluate_model(
    xavier_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the Kaiming initialization model
kaiming_model = build_kaiming_model()
history_kaiming, test_loss_kaiming, test_acc_kaiming = compile_fit_evaluate_model(
    kaiming_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the dropout regularization model
dropout_model = build_dropout_model()
history_dropout, test_loss_dropout, test_acc_dropout = compile_fit_evaluate_model(
    dropout_model, x_train, y_train, x_test, y_test
)

# Train and evaluate the L2 regularization model
regularized_model = build_regularized_model()
history_regularized, test_loss_regularized, test_acc_regularized = compile_fit_evaluate_model(
    regularized_model, x_train, y_train, x_test, y_test
)

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['accuracy'], label='Baseline Training Accuracy')
plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')
plt.plot(history_xavier.history['accuracy'], label='Xavier Training Accuracy')
plt.plot(history_xavier.history['val_accuracy'], label='Xavier Validation Accuracy')
plt.plot(history_kaiming.history['accuracy'], label='Kaiming Training Accuracy')
plt.plot(history_kaiming.history['val_accuracy'], label='Kaiming Validation Accuracy')
plt.plot(history_dropout.history['accuracy'], label='Dropout Training Accuracy')
plt.plot(history_dropout.history['val_accuracy'], label='Dropout Validation Accuracy')
plt.plot(history_regularized.history['accuracy'], label='Regularized Training Accuracy')
plt.plot(history_regularized.history['val_accuracy'], label='Regularized Validation Accuracy')
plt.title('Model Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim([0, 1])
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['loss'], label='Baseline Training Loss')
plt.plot(history_baseline.history['val_loss'], label='Baseline Validation Loss')
plt.plot(history_xavier.history['loss'], label='Xavier Training Loss')
plt.plot(history_xavier.history['val_loss'], label='Xavier Validation Loss')
plt.plot(history_kaiming.history['loss'], label='Kaiming Training Loss')
plt.plot(history_kaiming.history['val_loss'], label='Kaiming Validation Loss')
plt.plot(history_dropout.history['loss'], label='Dropout Training Loss')
plt.plot(history_dropout.history['val_loss'], label='Dropout Validation Loss')
plt.plot(history_regularized.history['loss'], label='Regularized Training Loss')
plt.plot(history_regularized.history['val_loss'], label='Regularized Validation Loss')
plt.title('Model Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['accuracy'], label='Baseline Training Accuracy')
plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')
plt.plot(history_xavier.history['accuracy'], label='Xavier Training Accuracy')
plt.plot(history_xavier.history['val_accuracy'], label='Xavier Validation Accuracy')
plt.plot(history_kaiming.history['accuracy'], label='Kaiming Training Accuracy')
plt.plot(history_kaiming.history['val_accuracy'], label='Kaiming Validation Accuracy')
plt.plot(history_dropout.history['accuracy'], label='Dropout Training Accuracy')
plt.plot(history_dropout.history['val_accuracy'], label='Dropout Validation Accuracy')
plt.plot(history_regularized.history['accuracy'], label='Regularized Training Accuracy')
plt.plot(history_regularized.history['val_accuracy'], label='Regularized Validation Accuracy')
plt.title('Model Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim([0, 1])
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['loss'], label='Baseline Training Loss')
plt.plot(history_baseline.history['val_loss'], label='Baseline Validation Loss')
plt.plot(history_xavier.history['loss'], label='Xavier Training Loss')
plt.plot(history_xavier.history['val_loss'], label='Xavier Validation Loss')
plt.plot(history_kaiming.history['loss'], label='Kaiming Training Loss')
plt.plot(history_kaiming.history['val_loss'], label='Kaiming Validation Loss')
plt.plot(history_dropout.history['loss'], label='Dropout Training Loss')
plt.plot(history_dropout.history['val_loss'], label='Dropout Validation Loss')
plt.plot(history_regularized.history['loss'], label='Regularized Training Loss')
plt.plot(history_regularized.history['val_loss'], label='Regularized Validation Loss')
plt.title('Model Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['accuracy'], label='Baseline Training Accuracy')
plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')
plt.plot(history_xavier.history['accuracy'], label='Xavier Training Accuracy')
plt.plot(history_xavier.history['val_accuracy'], label='Xavier Validation Accuracy')
plt.plot(history_kaiming.history['accuracy'], label='Kaiming Training Accuracy')
plt.plot(history_kaiming.history['val_accuracy'], label='Kaiming Validation Accuracy')
plt.plot(history_dropout.history['accuracy'], label='Dropout Training Accuracy')
plt.plot(history_dropout.history['val_accuracy'], label='Dropout Validation Accuracy')
plt.plot(history_regularized.history['accuracy'], label='Regularized Training Accuracy')
plt.plot(history_regularized.history['val_accuracy'], label='Regularized Validation Accuracy')
plt.title('Model Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim([0, 1])
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(history_baseline.history['loss'], label='Baseline Training Loss')
plt.plot(history_baseline.history['val_loss'], label='Baseline Validation Loss')
plt.plot(history_xavier.history['loss'], label='Xavier Training Loss')
plt.plot(history_xavier.history['val_loss'], label='Xavier Validation Loss')
plt.plot(history_kaiming.history['loss'], label='Kaiming Training Loss')
plt.plot(history_kaiming.history['val_loss'], label='Kaiming Validation Loss')
plt.plot(history_dropout.history['loss'], label='Dropout Training Loss')
plt.plot(history_dropout.history['val_loss'], label='Dropout Validation Loss')
plt.plot(history_regularized.history['loss'], label='Regularized Training Loss')
plt.plot(history_regularized.history['val_loss'], label='Regularized Validation Loss')
plt.title('Model Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

def compile_fit_evaluate_model(model, x_train, y_train, x_test, y_test):
    """Compiles, fits, and evaluates a Keras model."""
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        x_train, y_train,
        validation_data=(x_test, y_test),
        epochs=10,
        batch_size=128,
        verbose=0 # Set verbose to 0 to avoid printing progress bar for each epoch
    )

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

    return history, test_loss, test_acc

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import regularizers

def build_baseline_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_xavier_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(64, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(32, activation='relu', kernel_initializer='glorot_uniform'),
        layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
    ])
    return model

def build_kaiming_model():
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(10, activation='softmax', kernel_initializer='he_normal')
    ])
    return model

def build_dropout_model(dropout_rate=0.3):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(64, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(32, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    return model

def build_regularized_model(l2_factor=0.001):
    model = tf.keras.Sequential([
        layers.Input(shape=(32, 32, 3)),
        layers.Flatten(),
        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)),
        layers.Dense(10, activation='softmax') # Regularization is typically not applied to the output layer
    ])
    return model

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import random
from tensorflow.keras import layers, models

"""LOAD DATASET"""

cifar10 = tf.keras.datasets.cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)

"""NORMALISE"""

x_train, x_test = x_train / 255.0, x_test / 255.0

y_train, y_test = y_train.flatten(), y_test.flatten()

class_names=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i])
    plt.xlabel(class_names[y_train[i]])
plt.show()

"""BUILD THE MODEL"""

import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential([
    layers.Input(shape=(32, 32, 3)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),

    layers.Dense(10, activation='softmax')
])

model.summary()

"""COMPLIE THE MODEL"""

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

"""TRAIN THE MODEL"""

history = model.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=10,
    batch_size=128
)

"""PLOT THE ACCURACY

"""

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.1, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('loss')
plt.ylim([1,2])
plt.legend()

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)

print(test_loss)

print(test_acc)

"""PREDICT THE IMAGE


"""

n= random.randint(0, len(x_test) - 1)

pred = model.predict(np.expand_dims(x_test[n], axis=0))
predicted_class = np.argmax(pred)

# Show the image with prediction and true label
plt.imshow(x_test[n])
plt.title(f"Predicted: {class_names[predicted_class]}\nActual: {class_names[y_test[n]]}")

plt.show()

"""# Task
Compare the performance of a baseline neural network model with models using Xavier initialization, Kaiming initialization, dropout regularization, and L1 or L2 regularization on a given dataset within the provided notebook environment.

## Define model creation functions

### Subtask:
Create separate functions to build the model with the following configurations:
    - Baseline model (default initializer)
    - Model with Xavier initialization (`glorot_uniform`)
    - Model with Kaiming initialization (`he_normal`)
    - Model with dropout regularization (apply dropout to hidden layers)
    - Model with L1 or L2 regularization (add regularization to dense layers)

**Reasoning**:
Define functions to build different model architectures with specified initializations and regularizations as requested.

## Define training and evaluation function

### Subtask:
Create a function that compiles, fits, and evaluates a given model.

**Reasoning**:
Define a function to compile, fit, and evaluate a given Keras model as requested in the instructions.

## Train and evaluate each model

### Subtask:
Call the training and evaluation function for each model created in step 1. Store the history and evaluation results for each model.

**Reasoning**:
Call the training and evaluation function for each model and store the results.

## Visualize the results

### Subtask:
Plot the training and validation accuracy and loss curves for all models on the same plots for comparison.

**Reasoning**:
Create plots for accuracy and loss, adding all model histories to the respective plots for comparison.

**Reasoning**:
The previous code block failed because `plt` was not defined. Import `matplotlib.pyplot` and regenerate the plotting code.

**Reasoning**:
The error indicates that the history objects from the model training step are not available in the current environment. Re-execute the training and evaluation step before plotting.

**Reasoning**:
The previous code block failed because the model building and training functions were not defined in the current session. Re-execute the code cells that define these functions and then the code cell that trains the models and plots the results.

**Reasoning**:
The data variables (x_train, y_train, x_test, y_test) were not defined in the current session. Re-execute the code cells that load and preprocess the data, then the code cell that defines the model building and training functions, and finally the code cell that trains the models and plots the results.
"""